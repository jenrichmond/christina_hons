---
title: "read_data"
author: "Jen Richmond"
date: "16/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The point of this Rmd is to try and reproduce the cleaning process outlined in the first script /scripts/kidwell_et_al.(2016)/Actual Availability, All Journals.R

# load packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(janitor)
library(lubridate)
```

# read data

The script above reads data straight from OSF (clever) but I wrote it to the data_files folder to make it easy to read back in. 

```{r}
kidwell <- read_csv(here::here("data_files", "master_dataset.csv")) %>%
  clean_names()

```

# remove articles that are not empirical

```{r}
empirical_kidwell <- kidwell %>%
  filter(number_of_experiments > 0)
```

# assign journal name

Found this case_when + str_detect  combo came from https://bookdown.org/ansellbr/WEHI_tidyR_course_book/manipulating-data-with-dplyr.html#


I worked out here that order is important. If you put PS as the first case_when, it only ends up capturing 3 journals because it counts CPS and JPSP as PS. Important to put PS as the last case it evaluates. 



```{r}
empirical_kidwell <- empirical_kidwell %>%
  mutate(journal = case_when( 
                             str_detect(article_id_number,'CPS') ~ "Clinical Psychological Science", 
                             str_detect(article_id_number,'DP') ~ "Developmental Psychology", 
                             str_detect(article_id_number,'JEPLMC') ~ "Journal of Experimental Psychology: Learning, Memory, and Cognition", 
                             str_detect(article_id_number,'JPSP') ~ "Journal of Personality and Social Psychology", 
                             str_detect(article_id_number,'PS') ~ "Psychological Science",
                                      TRUE ~ "other")) %>%
  mutate(year = case_when(str_detect(article_id_number,'2012') ~ "2012", 
                             str_detect(article_id_number,'2013') ~ "2013", 
                             str_detect(article_id_number,'2014') ~ "2014", 
                             str_detect(article_id_number,'2015') ~ "2015", 
                             TRUE ~ "other")) %>%
  relocate(journal, .after = article_id_number) %>%
  relocate(year, .before = article_id_number)

unique(empirical_kidwell$journal)
unique(empirical_kidwell$year)

```
# get date out of article id

```{r}
empirical_kidwell <- empirical_kidwell %>%
  separate(article_id_number, into = c("date", "journal_code"), sep = "\\s", remove = FALSE)

```

```{r}
# change class to date 
empirical_kidwell$date <- dmy(empirical_kidwell$date)

#check class
class(empirical_kidwell$date)
```

25 failed to parse, which ones? arrange by date and then select the botton (ie tail) 25 rows. Hmmmm those are not possible dates!

```{r}

which <- empirical_kidwell %>%
  arrange(date) %>%
  tail(25)
```

# check papers per journal
```{r}
journal_year <- empirical_kidwell %>%
  tabyl(journal, year) 

jy_long <- journal_year %>%
  pivot_longer(names_to = "year", values_to = "total", 2:5)
```

# open access data

filter for links to repo, personal, third party site. 
```{r}
openaccess_data <- empirical_kidwell %>%
  filter(data_url_links_to %in% c("Independent archive / repository", "Personal site", "Third party site"))

```

Count how many open access in each journal each year
```{r}
open_journal_year <- openaccess_data %>%
  tabyl(journal, year) 

ojy_long <- open_journal_year %>%
  pivot_longer(names_to = "year", values_to = "open", 2:5)
```


# join total + open df

```{r}
open_total <- left_join(jy_long, ojy_long) %>%
  mutate(open = replace_na(open, 0)) %>%
  mutate(percent_open = (open/total)*100)
```
# plot percent open by year
```{r}
open_total %>%
  ggplot(aes(x = year, y = percent_open, colour = journal, group = journal)) +
  geom_point() + 
  geom_line() +
  scale_colour_discrete(name = "journal", labels = c("CPS", "DP", "JECPLMC", "JPSP", "PS"))
```



# actually locatable

in the original code there was a loop calculating for each year how many papers that got a open data badge had data available at the location specified. 

key variables in the loop
- Did.the.article.receive.a.badge.for.open.data
- Are.the.data.located.at.the.working.page


```{r}
loop <- openaccess_data %>%
  select(1:6, did_the_article_receive_a_badge_for_open_data, are_the_data_located_at_the_working_page) %>%
  rename(badge = did_the_article_receive_a_badge_for_open_data, 
         located = are_the_data_located_at_the_working_page)

```

```{r}

located <- loop %>%
  group_by(year, journal, badge) %>%
  count(located) %>%
  arrange(year, journal)
```

